# Grid search hyper-parameter(lambda, intercept) tuning for l2svm 

l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B, Boolean icpt) 
return (Matrix[Double] loss) {
  if (icpt)
    X = cbind(X, matrix(1, nrow(X), 1));
  loss = as.matrix(sum((y - X%*%B)^2));
}

imputeByMean = function(Matrix[Double] X) return(Matrix[Double] X)
{
  Mask = is.nan(X);
  X = replace(target=X, pattern=NaN, replacement=0)
  Mask = Mask * (colMeans(X))
  X = X + Mask;
}

readNdClean = function() return (Matrix[double] X, Matrix[double] y)
{
  Forig = read("../datasets/KDD98.csv", data_type="frame", format="csv", header=TRUE);
  F = Forig[,1:469];

  # discretize target vector for classification
  median = median(as.matrix(Forig[,472]));
  y = ifelse(as.matrix(Forig[,472]) > median, 2, 1);

  # data preparation
  bin = matrix(0, ncol(F), 1);
  bin[5,] = 1;
  bin[8,] = 1;
  bin[17,] = 1;
  bin[27,] = 1;
  bin[44:50,] = matrix(1, 7, 1);
  bin[54,] = 1;
  bin[76:195,] = matrix(1, 195-76+1, 1);
  bin[199:361,] = matrix(1, 361-199+1, 1);
  bin[408,] = 1;
  bin[410:412,] = matrix(1, 3, 1);
  bin[435:469,] = matrix(1, 469-435+1, 1);

  recode="1";
  for(i in 2:nrow(bin))
    if( as.scalar(bin[i,])!=1 )
      recode = recode+","+i;
  binning = "{id:5, method:equi-width, numbins:10}"
  for(i in 6:nrow(bin))
    if( as.scalar(bin[i,])==1 )
      binning = binning+",\n{id:"+i+", method:equi-width, numbins:10}";
  dummy="1";
  for (i in 2:nrow(bin))
    if( as.scalar(bin[i,])!=1 | as.scalar(bin[i,])==1)
      dummy = dummy+","+i;

  jspec= "{ ids:true, recode:["+recode+"], bin:["+binning+"], dummycode:["+dummy+"]}"
  [X,M] = transformencode(target=F, spec=jspec);
  X = replace(target=X, pattern=NaN, replacement=0);
  print("number of rows "+nrow(X) + "\nnumber of cols " + ncol(X));
}

######################################################

dataset = $1;
if (dataset == "KDD_noprep") {
  # Read already prepared KDD dataset
  X = read("../datasets/KDD_X", format="binary");
  y = read("../datasets/KDD_y.csv", format="csv");
}

if (dataset == "KDD_prep")
  # Read raw KDD dataset, clean, transform and convert to matrix 
  [X, y] = readNdClean();

if (dataset == "KDD_synthetic") {
  # Read KDD-equivalent synthetic dataset
  X = read("../datasets/KDD_X_Syn.csv", header=FALSE);
  y = read("../datasets/KDD_y_Syn.csv", header=FALSE);
}

#discretize target vector for classification
median = median(y);
y = ifelse(y > median, 2, 1);

M = nrow(X);
N = ncol(X);
no_lamda = 70;
stp = (0.1 - 0.0001)/no_lamda;
lamda = 0.0001;
Rbeta = matrix(0, rows=N+1, cols=no_lamda*2);
Rloss = matrix(0, rows=no_lamda*2, cols=1);
i = 1;

for (l in 1:no_lamda)
{
  # Call with intercept False
  beta = l2svm(X=X, Y=y, intercept=FALSE, epsilon=1e-12, 
       maxIterations=10, maxii=4, verbose=FALSE);
  Rbeta[1:nrow(beta),i] = beta;
  Rloss[i,] = l2norm(X, y, beta, FALSE);
  i = i + 1;

  # Call with intercept True
  beta = l2svm(X=X, Y=y, intercept=TRUE, epsilon=1e-12, 
       maxIterations=10, maxii=4, verbose=FALSE);
  Rbeta[1:nrow(beta),i] = beta;
  Rloss[i,] = l2norm(X, y, beta, TRUE);
  i = i + 1;

  lamda = lamda + stp;
}

leastLoss = rowIndexMin(t(Rloss));
bestModel = Rbeta[,as.scalar(leastLoss)];
write(bestModel, "outdml", format="binary");
